{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Workspace\r\n",
    "from azureml.core import Experiment\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\r\n",
    "from azureml.core.webservice import AksWebservice, Webservice\r\n",
    "from azureml.core.environment import Environment\r\n",
    "from azureml.core.webservice import Webservice\r\n",
    "from azureml.core.model import InferenceConfig\r\n",
    "from azureml.core.model import Model\r\n",
    "from azureml.core.compute import AksCompute\r\n",
    "from azureml.core.webservice import AksWebservice, Webservice\r\n",
    "from azureml.core.webservice import AksEndpoint\r\n",
    "from azureml.core.model import Model\r\n",
    "from azureml.core.compute import AksCompute\r\n",
    "from sklearn.linear_model   import LogisticRegression\r\n",
    "from sklearn.preprocessing  import StandardScaler\r\n",
    "from sklearn.ensemble       import RandomForestClassifier\r\n",
    "from sklearn.metrics        import accuracy_score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import azureml.core\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core import Experiment\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "import joblib\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ws = Workspace.from_config()\r\n",
    "experiment = Experiment(workspace=ws, name=\"azureml-iris-experiment\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Data for training\r\n",
    "iris_df = pd.read_csv('./data/iris.csv')\r\n",
    "iris_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Label Encoding (Gender)\r\n",
    "iris_df['species'] = iris_df.species.map( {'setosa': 0, 'virginica': 1, 'versicolor': 2} ).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Select Feature and Target\r\n",
    "Feature = iris_df.drop(columns=['species'], axis=1)\r\n",
    "Target  = iris_df['species']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train Test Split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(Feature, Target, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "estimators = [5, 10, 20, 50, 100]\r\n",
    "\r\n",
    "for estimator in estimators:\r\n",
    "    run = experiment.start_logging()\r\n",
    "    run.log(\"estimator_value\", estimator)\r\n",
    "    \r\n",
    "    # Random Forest classifier\r\n",
    "    classifier = RandomForestClassifier(n_estimators=estimator, random_state=42)\r\n",
    "    classifier.fit(X_train, y_train)\r\n",
    "    \r\n",
    "    y_pred = classifier.predict(X_test)\r\n",
    "    \r\n",
    "    accuracy = accuracy_score(y_test, y_pred)\r\n",
    "    run.log('Accuracy', accuracy)\r\n",
    "    \r\n",
    "    model_name = \"rf_model_est_\" + str(estimator) + \".pkl\"\r\n",
    "    filename = \"outputs/\" + model_name\r\n",
    "    \r\n",
    "    joblib.dump(value=classifier, filename=f'outputs/rf_model_est_{str(estimator)}.pkl')\r\n",
    "    \r\n",
    "    run.upload_file(name=model_name, path_or_stream=filename)\r\n",
    "    run.complete()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the best model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_acc_runid = None\r\n",
    "best_acc = None\r\n",
    "\r\n",
    "best_modeldf = pd.DataFrame()\r\n",
    "\r\n",
    "for run in experiment.get_runs():\r\n",
    "    run_metrics = run.get_metrics()\r\n",
    "    run_details = run.get_details()\r\n",
    "#     best_modeldf = best_modeldf.append([run_metrics])\r\n",
    "#     print(run_details)\r\n",
    "# Choose best estomator\r\n",
    "    \r\n",
    "    # each logged metric becomes a key in this returned dict\r\n",
    "    try:\r\n",
    "        run_acc = run_metrics[\"Accuracy\"]\r\n",
    "        run_id = run_details[\"runId\"]\r\n",
    "    except:\r\n",
    "        run_acc = 0\r\n",
    "        run_id  = 999\r\n",
    "    \r\n",
    "    if best_acc is None:\r\n",
    "        best_acc = run_acc\r\n",
    "        best_acc_runid = run_id\r\n",
    "    else:\r\n",
    "        if run_acc > best_acc:\r\n",
    "            best_acc = run_acc\r\n",
    "            best_acc_runid = run_id\r\n",
    "\r\n",
    "print(\"Best run_id: \" + best_acc_runid)\r\n",
    "print(\"Best run_id accuracy: \" + str(best_acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Choose Best Estimator\r\n",
    "best_run = Run(experiment=experiment, run_id=best_acc_runid)\r\n",
    "best_run\r\n",
    "print(best_run.get_file_names())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# best_run.download_file(name=\"rf_model_est_100.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run.complete()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# register model\r\n",
    "model = run.register_model(model_name='sklearn_iris_model',\r\n",
    "                           model_path='outputs/rf_model_est_5.pkl')\r\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile score.py\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import joblib\r\n",
    "\r\n",
    "def init():\r\n",
    "    global model\r\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\r\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\r\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\r\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'rf_model_est_5.pkl')\r\n",
    "    model = joblib.load(model_path)\r\n",
    "\r\n",
    "def run(raw_data):\r\n",
    "    predict_map = {0:'setosa', 1:'virginica', 2:'versicolor'}\r\n",
    "    data = np.array(json.loads(raw_data)['data']).reshape(-1, 1)\r\n",
    "    # make prediction\r\n",
    "    y_hat = model.predict(data)\r\n",
    "    # you can return any data type as long as it is JSON-serializable\r\n",
    "    return predict_map[y_hat.tolist()[0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create ACI Instance & Deploy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.webservice import AciWebservice\r\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \r\n",
    "                                               memory_gb=1, \r\n",
    "                                               tags={\"data\": \"IRIS\",  \"method\" : \"sklearn\"}, \r\n",
    "                                               description='Predict IRIS with sklearn')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "import uuid\r\n",
    "from azureml.core.webservice import Webservice\r\n",
    "from azureml.core.model import InferenceConfig\r\n",
    "from azureml.core.environment import Environment\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core.model import Model\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "model = Model(ws, id='sklearn_iris_model:1') # We can get this from model section\r\n",
    "\r\n",
    "\r\n",
    "myenv = Environment.get(workspace=ws, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\", version=\"10\")\r\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\r\n",
    "\r\n",
    "service_name = 'test-sklearn-iris-' + str(uuid.uuid4())[:4]\r\n",
    "service = Model.deploy(workspace=ws, \r\n",
    "                       name=service_name, \r\n",
    "                       models=[model], \r\n",
    "                       inference_config=inference_config, \r\n",
    "                       deployment_config=aciconfig)\r\n",
    "\r\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(service.scoring_uri)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Sending Raw HTTPS request\r\n",
    "import requests\r\n",
    "\r\n",
    "# send a random row from the test set to score\r\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test.iloc[10])) + \"]}\"\r\n",
    "\r\n",
    "headers = {'Content-Type':'application/json'}\r\n",
    "\r\n",
    "# for AKS deployment you'd need to the service key in the header as well\r\n",
    "# api_key = service.get_key()\r\n",
    "# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \r\n",
    "\r\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\r\n",
    "\r\n",
    "print(\"POST to url\", service.scoring_uri)\r\n",
    "#print(\"input data:\", input_data)\r\n",
    "print(\"label:\", y_test.iloc[10])\r\n",
    "print(\"prediction:\", resp.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test.iloc[10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"{\\\"data\\\": [\" + str(list(X_test.iloc[10])) + \"]}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}